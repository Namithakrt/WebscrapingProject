{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f487439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install selenium\n",
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9805ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781027c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#setting up headless chrome webdriver\n",
    "def make_driver():\n",
    "    options=webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "driver=make_driver()\n",
    "url=\"https://www.ycombinator.com/companies\"\n",
    "#opens the specified URL's in the chrome webdriver\n",
    "def get_url():\n",
    "    driver.get(url)\n",
    "#function to click the see all options button in batch\n",
    "def click_see_all_options():\n",
    "    time.sleep(5)\n",
    "    see_all_options= driver.find_element(By.CSS_SELECTOR, 'a._showMoreLess_99gj3_241')\n",
    "    see_all_options.click()\n",
    "#function to filter checkboxes representing batches\n",
    "def compile_batches():\n",
    "    pattern=re.compile(r'^(|W|S|IK)[012]')\n",
    "    filter_checkbox=driver.find_elements(By.XPATH,'//label')\n",
    "    for checkbox in filter_checkbox:\n",
    "        if pattern.match(checkbox.text):\n",
    "            yield checkbox\n",
    "#function to scroll down the webpage until it reaches the bottom\n",
    "def scroll_to_bottom(driver):\n",
    "    last_height=driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height=driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height==last_height:\n",
    "            break\n",
    "        last_height=new_height\n",
    "#function to extract company URL's from webpage        \n",
    "def fetch_url_paths():\n",
    "    elements = driver.find_elements(\n",
    "        By.XPATH, ('//a[contains(@href,\"/companies/\") and not(contains(@href,\"founders\"))]'))\n",
    "    for url in elements:\n",
    "        yield url.get_attribute('href')\n",
    "#function to write extracted url's to a file\n",
    "def write_urls_to_file(url_list):\n",
    "    with open('company_urls.txt', 'w') as f:\n",
    "        for url in url_list:\n",
    "            f.write(url + '\\n')\n",
    "#function to read url's from the file\n",
    "def read_urls_from_file():\n",
    "    url_list = [] \n",
    "    with open('company_urls.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            url_list.append(line.strip())\n",
    "    return url_list   \n",
    "#main function to extract company url's and save company url's\n",
    "def yc_links_extractor():\n",
    "    try:\n",
    "        get_url()\n",
    "        click_see_all_options()\n",
    "        batches=compile_batches()\n",
    "        all_company_urls=[]\n",
    "    \n",
    "        for b in tqdm(list(batches)):\n",
    "            b.click()\n",
    "            scroll_to_bottom(driver)\n",
    "            urls = fetch_url_paths()\n",
    "            all_company_urls.extend(urls)\n",
    "            b.click()\n",
    "        write_urls_to_file(all_company_urls)\n",
    "        print(f\"Total company URLs extracted: {len(all_company_urls)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    yc_links_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892383ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (201942027.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 33\u001b[1;36m\u001b[0m\n\u001b[1;33m    website_link=soup.find('div',class_=\"inline-block group-hover:underline\")\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#function to scrape company website and extract company details\n",
    "def scrape_company_website(url):\n",
    "    try:\n",
    "        response=requests.get(url)   #send an ghttp get request to the url\n",
    "        response.raise_for_status()  #raise an exception for unsuccessful response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while requesting {url}: {e}\")  # prints error message if request fails\n",
    "        return None\n",
    "    #parse the html content using BeautifulSoup\n",
    "    if response.status_code==200:\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        #extract company name\n",
    "        company_name=soup.find('h1',class_=\"font-extralight\").text.strip()\n",
    "        #extract tagline\n",
    "        tagline=soup.find('h3',class_=\"sm:block md:hidden\").text.strip()\n",
    "        #extract description of the company\n",
    "        description=[]\n",
    "        description_tags=soup.find('p',class_=\"whitespace-pre-line\")\n",
    "        if description_tags:\n",
    "            descriptions=description_tags.get_text(separator=\"\",strip=True)\n",
    "            description=descriptions.split('\\n')[0]  #gets the first line\n",
    "        else:\n",
    "            print(\"description not found\")\n",
    "        #extracts batch, industry tags and company type\n",
    "        batches= soup.find('a', href=lambda href: href and 'batch'in href)\n",
    "        batch=batches.find('span').text.strip() if batches else None\n",
    "        industry_tags = [a.text.strip() for a in soup.find_all('a', href=lambda href: href and 'industry' in href)]\n",
    "        company_type = soup.find('div', class_='flex flex-row items-center justify-between').text.strip()\n",
    "        #extract location,website link,founded year and team size\n",
    "        location=soup.find('span',string=\"Location:\")\n",
    "        locations=location.find_next_sibling('span').text.strip()  (#text after \"Location:\" span)\n",
    "        \n",
    "        website_link=soup.find('div',class_=\"inline-block group-hover:underline\")\n",
    "        website=website_link.text.strip() if website_link else None\n",
    "\n",
    "        founded=soup.find('span',string=\"Founded:\")\n",
    "        founded_year=founded.find_next_sibling('span').text.strip()   (#text after \"Founded:\" span)\n",
    "        \n",
    "        team_size=soup.find('span',string=\"Team Size:\")\n",
    "        team_size_value=team_size.find_next_sibling('span').text.strip()   (#text after \"Team Size:\" span)\n",
    "        #extract social media profiles of company\n",
    "        social_media=soup.find('div',class_=\"space-x-2\")\n",
    "        social_media_url={}\n",
    "        if social_media:\n",
    "            links=social_media.find_all('a')\n",
    "            for link in links:\n",
    "                if 'linkedin' in link['href']:\n",
    "                    social_media_url['linkedin']=link['href']\n",
    "                elif 'twitter' in link['href']:\n",
    "                    social_media_url['twitter']=link['href']\n",
    "                elif 'facebook' in link['href']:\n",
    "                    social_media_url['facebook']=link['href']\n",
    "                elif 'crunchbase' in link['href']:\n",
    "                    social_media_url['crunchbase']=link['href']\n",
    "        #extracts founder name\n",
    "        founder_name=[]\n",
    "        founder_tags=soup.find('h3',class_=\"text-lg font-bold\")\n",
    "        if founder_tags:\n",
    "            tags=founder_tags.text.strip()\n",
    "            name=tags.split(',',1)[0] if ',' in tags else tags.strip()\n",
    "            founder_name.append(name)\n",
    "        #extracts founder biography\n",
    "        biography=[]\n",
    "        biographies=soup.find('p',class_=\"prose max-w-full whitespace-pre-line\")\n",
    "        if biographies:\n",
    "            biography=[bio.text.strip().split('\\n')[:1] for bio in biographies]  #extracts first line from biography\n",
    "        else:\n",
    "            print(\"bio not found\")\n",
    "        #extracts social media profiles of founder   \n",
    "        social_media_profile=soup.find('div',class_=\"mt-1 space-x-2\")\n",
    "        social_media_profile_data={}\n",
    "        if social_media_profile:\n",
    "            links=social_media_profile.find_all('a')\n",
    "            for link in links:\n",
    "                if 'linkedin' in link['href']:\n",
    "                    social_media_profile_data['linkedin']=link['href']\n",
    "                elif 'twitter' in link['href']:\n",
    "                    social_media_profile_data['twitter']=link['href']\n",
    "        #  create dictionaries for founder information and company data\n",
    "        founders_info=[]\n",
    "        founder_data = {\n",
    "                    \"Name\":founder_name,\n",
    "                    \"Biography\": biography,\n",
    "                    \"LinkedIn Profile\": social_media_profile_data.get('linkedin'),\n",
    "                    \"Twitter Profile\": social_media_profile_data.get('twitter')\n",
    "                }\n",
    "        founders_info.append(founder_data)\n",
    "                    \n",
    "        company_data={\n",
    "           \"company\":{\n",
    "              \"Company Name\": company_name,\n",
    "              \"Tagline\": tagline,\n",
    "              \"Description\":description,\n",
    "              \"batch\":batch,\n",
    "              \"company type\":company_type,\n",
    "              \"industry tags\":industry_tags,\n",
    "              \"Location\": locations,\n",
    "              \"website\":website,\n",
    "              \"Founded\": founded_year,\n",
    "              \"Team Size\": team_size_value,\n",
    "              \"social media profiles\":{\n",
    "                    \"LinkedIn\": social_media_url.get('linkedin'),\n",
    "                    \"Twitter\": social_media_url.get('twitter'),\n",
    "                    \"Facebook\": social_media_url.get('facebook'),\n",
    "                    \"Crunchbase\": social_media_url.get('crunchbase')\n",
    "              }\n",
    "           },\n",
    "            \"founders\":founders_info\n",
    "        \n",
    "    }  \n",
    "        return company_data  #return the dictionary containing all the scraped company data\n",
    "#function to save data to json file   \n",
    "def save_to_json(company_data,filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        json.dump(company_data,f, indent=4)\n",
    "        f.write('\\n')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    all_company_urls=read_urls_from_file()  #read company urls from file\n",
    "    total_urls = len(all_company_urls)\n",
    "    with tqdm(total=total_urls,desc=\"scraping\")as pbar: #progress bar for tracking completion\n",
    "        for url in all_company_urls:\n",
    "            company_data=scrape_company_website(url)\n",
    "            save_to_json(company_data,\"company_data.json\")  #save all scraped data to json file\n",
    "            pbar.update(1)\n",
    "       \n",
    "    #prints confirmation message        \n",
    "    print(\"Scraping completed. Data saved to 'company_data.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf10bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cf396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81660a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
